








from platform import  python_version


python_version()


import sys


# # Uncomment these if any packages are not installed in your current jupyter env 
# # Installing a pip package in the current kernel
# # Pandas also installs the numpy package
# !{sys.executable} -m pip install pandas  
# !{sys.executable} -m pip install requests
# !{sys.executable} -m pip install matplotlib
# !{sys.executable} -m pip install sklearn
# !{sys.executable} -m pip install featuretools
# !{sys.executable} -m pip install seaborn
# !{sys.executable} -m pip install jupyterlab-citation-manager


# import the required packages (more imported in the model section)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import os
import datetime as dt
import featuretools as ft
from featuretools.selection import selection
from IPython.core.interactiveshell import InteractiveShell 
from IPython.display import Image  
import inspect
from functools import reduce


# import warnings
# warnings.filterwarnings('ignore')


def view_files():
    path = os.getcwd()
    path = f"{path}\data"
    return(os.listdir(path))

def check_file(file):
    #grab the file path from which to import the dataset
    path = os.getcwd()
    path = f"{path}\data"
    path = f"{path}\{file}"
    return(path)

def import_data(path):
    # read the csv file as a dataframe and remove unnecessary columns
    df = pd.read_csv(filepath_or_buffer=path)
    return(df)


view_files()


check_file("absa.csv")


absa = import_data(check_file("absa2.csv"))
absa.head(1)


def clean_names1(df, bank_prefix="B_34118: "):
    df = df.drop(["Unit", "Time series code"], axis="columns")
    
    # Remove ugly string labels of columns
    df.columns = df.columns.str.replace(pat="D_M_[0-9]{4}M[0-9]{2}:", repl="", regex=True)
    df["Bank"] = df["Bank"].str.replace(pat=bank_prefix, repl="", regex=True)
    df["Bank"] = df["Bank"].str.replace(pat = " ", repl = "_")
    return(df)
    


absa = clean_names1(absa)


absa.head(1)


InteractiveShell.ast_node_interactivity = "all"


all(absa[absa["Table"].str.contains("T_T[0-9]{2}R[0-9]{3}:", 
                                    regex = True)].isna().sum(axis=1, 
                                                              skipna=False) == len(absa.columns) - 2);
all(absa[absa["Table"].str.contains("T_T[0-9]{2}R[0-9]{3}_A:", 
                                    regex = True)].isna().sum(axis=1, 
                                                              skipna=False) == len(absa.columns) - 2)


InteractiveShell.ast_node_interactivity = "last"


def remove_empty(df):
    # Remove empty title rows
    df = df[~df["Table"].str.contains("T_T[0-9]{2}R[0-9]{3}:", regex = True)]
    df = df[~df["Table"].str.contains("T_T[0-9]{2}R[0-9]{3}_A:", regex = True)]
    
    return(df)


absa = remove_empty(absa)


absa.head(1)


# absa.loc[absa["Table"].str.contains("T_T01R[0-9]{3}C[0-9]{2}: T01R[0-9]{3}[A]{0,1}C[0-9]{2}: ", 
#                                regex=True), :].iloc[:,1:3].set_index("Table").head(15)

absa.loc[absa["Table"].str.contains("T_T01R001|T_T01R002|T_T02R032", 
                               regex=True), :].iloc[:,1:3].set_index("Table")


66034650.0 + 2282.0 == 66036932.0

# i.e. what we have above is
r_001 = absa.loc[absa["Table"].str.contains("DEPOSITS [(]total of items 2 and 32[)]: TOTAL [(]7[)]"),
        :].iloc[:,3]
r_002 = absa.loc[absa["Table"].str.contains("DEPOSITS DENOMINATED IN RAND [(]total of items 3, 6, 12, 13 and 29[)]: TOTAL [(]7[)]"),
          :].iloc[:,3]
r_032 = absa.loc[absa["Table"].str.contains("DEPOSITS DENOMINATED IN FOREIGN CURRENCY [(]total of items 33 to 38[)]: TOTAL [(]7[)]"),
          :].iloc[:,3]
int(r_001) == int(r_002) + int(r_032)



def clean_names2(df):
    # Label the different tables withing the df, i.e liablities, assets, etc.
    df["Table"] = df["Table"].str.replace("T_T0[1-2](R[0-9]{3,4})C[0-9]{2}: T0[1-2]R[0-9]{3}[A]{0,1}C[0-9]{2}: ", 
                                          regex = True, repl=r"L_T1-2_\1_")
    df["Table"] = df["Table"].str.replace("T_T0[3-4](R[0-9]{3,4})C[0-9]{2}: T0[3-4]R[0-9]{3}[A]{0,1}C[0-9]{2}: ", 
                                          regex = True, repl=r"L_T3-4_\1_")
    df["Table"] = df["Table"].str.replace("T_T0[5](R[0-9]{3,4})C[0-9]{2}: T0[5]R[0-9]{3}[A]{0,1}C[0-9]{2}: ", 
                                      regex = True, repl=r"E_T5_\1_")
    df["Table"] = df["Table"].str.replace("T_T0[6-9](R[0-9]{3,4})C[0-9]{2}: T0[6-9]R[0-9]{3}[A]{0,1}C[0-9]{2}: ", 
                                          regex = True, repl=r"A_T6-13_\1_")
    df["Table"] = df["Table"].str.replace("T_T1[0-3](R[0-9]{3,4})C[0-9]{2}: T1[0-3]R[0-9]{3}[A]{0,1}C[0-9]{2}: ", 
                                          regex = True, repl=r"A_T6-13_\1_")
    df["Table"] = df["Table"].str.replace(",", "")
    # remove bracket explanations
#     df["Table"] = df["Table"].str.replace("[(][0-9a-z\s,]{2,}[)][:] ", regex = True, repl="")
    return(df)


from IPython.display import Image
Image("./figures/missing_totals.png", width=700, height=700)


absa.loc[absa["Table"].str.contains("Central.{,3}Bank", regex=True, case=False),][["Table", 
                                                   " 2007M11", " 2007M12", " 2008M01", " 2008M02", " 2022M04"]]


absa.loc[absa["Table"].str.contains("Deposits, loans", 
                                    regex=True, 
                                    case=False),]#[["Table", " 2007M11", " 2007M12", " 2008M01", " 2008M02"]]


absa.loc[absa["Table"].str.contains("NON-FINANCIAL", 
                                    regex=True),][["Table", 
                                                   " 2007M11", " 2007M12", " 2008M01", " 2008M02"]]


absa.loc[absa["Table"].str.contains("OTHER ASSETS", 
                                    regex=True),][["Table", 
                                                   " 2007M11", " 2007M12", " 2008M01", " 2008M02"]]


test1 = absa.loc[absa["Table"].str.contains("OTHER ASSETS", 
                                    regex=True),].loc[2011," 1993M01" : " 2007M12"].fillna(0)
test2 = absa.loc[absa["Table"].str.contains("OTHER ASSETS", 
                                    regex=True),].loc[2015," 1993M01" : " 2007M12"].fillna(0)
#sum(test1 == test2), len(test1)
test1[test1 != test2], test2[test1 != test2]


def fill_totals(df):
    # CB money
    repl1 = df["Table"].str.contains("T_T06R103C24: T06R103C24:", regex=True, case=False)
    with1 = df["Table"].str.contains("T_T06R103C18: T06R103C18:", regex=True, case=False)

    df.loc[repl1,:] = df.loc[repl1,:].replace(list(df.loc[repl1,:].iloc[0,2:]), 1)
    df.loc[repl1, " 1993M01":] = df.loc[repl1, " 1993M01":].cumsum(axis=1)

    df.loc[repl1,:] = df.loc[repl1,:].replace(list(df.loc[repl1,:].iloc[0,2:]), 
                                                  list(df.loc[with1,:].iloc[0,2:]))
    # Deposits
    repl2 = df["Table"].str.contains("T_T06R110C24: T06R110C24:", regex=True, case=False)
    with2 = df["Table"].str.contains("T_T06R110C18: T06R110C18:", regex=True, case=False)
    joint1 = df.loc[with2,:].iloc[0,2:].add(df.loc[repl2,:].iloc[0,2:])[" 2008M01":]
    with2b = list(df.loc[repl2,:].iloc[0,2:][:" 2007M12"].append(joint1))

    df.loc[repl2,:] = df.loc[repl2,:].replace(list(df.loc[repl2,:].iloc[0,2:]), 
                                                  list(with2b))
    
    # NON-FINANCIAL ASSETS
    repl3 = df["Table"].str.contains("T_T13R258C24: T13R258C24:", regex=True, case=False)
    with3 = df["Table"].str.contains("T_T13R258C18: T13R258C18:", regex=True, case=False)
    
    df.loc[repl3,:] = df.loc[repl3,:].replace(list(df.loc[repl3,:].iloc[0,2:]), 1)
    df.loc[repl3, " 1993M01":] = df.loc[repl3, " 1993M01":].cumsum(axis=1)

    df.loc[repl3,:] = df.loc[repl3,:].replace(list(df.loc[repl3,:].iloc[0,2:]), 
                                            list(df.loc[with3,:].iloc[0,2:]))
    
    # OTHER ASSETS
    repl4 = df["Table"].str.contains("T_T13R267C24: T13R267C24:", regex=True, case=False)
    with4a = df["Table"].str.contains("T_T13R267C21: T13R267C21:", regex=True, case=False)
    with4b = df["Table"].str.contains("T_T13R267C18: T13R267C18:", regex=True, case=False)
    joint2 = df.loc[with4a,:].fillna(0).iloc[0,2:].add(df.loc[with4b,:].fillna(0).iloc[0,2:])

    df.loc[repl4,:] = df.loc[repl4,:].replace(list(df.loc[repl4,:].iloc[0,2:]), 
                                            list(joint2))
    
    return(df)


absa = fill_totals(absa)


absa = clean_names2(absa)


absa.head(1)


list(absa["Table"])[:6]


list(absa[absa["Table"].str.contains("total", case=False)]["Table"])[:6]


list(absa.loc[absa["Table"].str.contains("TOTAL"),:]["Table"])[:6]


def filter_totals(df):
    df = df.loc[df["Table"].str.contains("TOTAL"),:]
    df = df.loc[df["Table"].str.contains("[(]total of items ", regex=True), :]
    dups = df["Table"].str.replace(".*(R[0-9]{3}).*", regex=True, repl=r"\1").duplicated()
    df = df.loc[~dups,:]
    return(df)


absa = filter_totals(absa)


InteractiveShell.ast_node_interactivity = "all"


absa.head(1)
absa.info()


InteractiveShell.ast_node_interactivity = "last"


def reformat(df):
    df = df.drop("Bank", axis=1)
    df = df.set_index("Table").T
    df.columns.name = None
    df.index.name = "Date"
    df.index = pd.to_datetime(df.index, format=" %YM%m")
    df.columns = df.columns.str.replace(pat=" ", repl="_")
    df.columns = df.columns.str.replace(pat="total_of_items", repl="tot")
    df = df.apply(pd.to_numeric)
    
    return(df)


absa = reformat(absa)


def liabilities(df):
    df = df.iloc[:,df.columns.str.startswith(("L_"))]
    return(df)

def equity(df):
    df = df.iloc[:,df.columns.str.startswith(("E_"))]
    return(df)

def assets(df):
    df = df.iloc[:,df.columns.str.startswith(("A_"))]
    return(df)


absa_l = liabilities(absa)
absa_e = equity(absa)
absa_a = assets(absa)


list(absa_l.iloc[:,
                absa_l.columns.str.contains("L_T[13]-[24]_R[0-9]{3}_[A-Z]{3,}")].columns[1:-1])


list(absa_e.columns[:-2])


absa_a.iloc[:,absa_a.columns.str.contains("A_T6-13_R[0-9]{3}_[A-Z-]{4,}")].columns[:-1]


def clean_liab(df):
    keep = list(df.iloc[:,
                            df.columns.str.contains("L_T[13]-[24]_R[0-9]{3}_[A-Z]{3,}")].columns[1:-1])
    del keep[-2]
    df = df[keep]
    df.columns = df.columns.str.replace("T[13]-[24]_R[0-9]{3}_", "", regex=True)
    df.columns = df.columns.str.replace("_[(].*[)]:", "", regex=True)
    df.columns = df.columns.str.replace("_[(][0-9][)]", "", regex=True)
    return(df)

def clean_assets(df):
    keep = list(df.iloc[:,
                            df.columns.str.contains("A_T6-13_R[0-9]{3}_[A-Z-]{4,}")].columns[:-1])
    df = df[keep]
    df.columns = df.columns.str.replace("T6-13_R[0-9]{3}_", "", regex=True)
    df.columns = df.columns.str.replace("_[(].*[)]:", "", regex=True)
    df.columns = df.columns.str.replace("_[(][0-9][)]", "", regex=True)
    df.columns = df.columns.str.replace("_[(].*[)]", "", regex=True)
    return(df)

def clean_equity(df):
    keep = list(df.columns[:-2])
    df = df[keep]
    df.columns = df.columns.str.replace("T5_R[0-9]{3}_", "", regex=True)
    df.columns = df.columns.str.replace("_[(].*[)]:", "", regex=True)
    df.columns = df.columns.str.replace("_[(][0-9][)]", "", regex=True)
    return(df)


absa_l = clean_liab(absa_l)
absa_e = clean_equity(absa_e)
absa_a = clean_assets(absa_a)


def join_diff(liab, ass, eq, bank="ABSA"):
    df = pd.concat([liab, ass, eq], axis = 1)
#     df = df.replace(0, 1)
#     df = df.fillna(1)
    df = df.fillna(0)
#     df = np.log(df)
    df = df.diff()
    df = df.iloc[1:,:]
    df.insert(0, "Bank", bank)
    return(df)


absa = join_diff(absa_l, absa_a, absa_e)


check_file("absa2.csv")


# Call the following to see a clear picture of the sequence of 
# steps to clean each of the banks' datasets.
[ f for f in globals().values() if inspect.isfunction(f) ]


view_files()


check_file("standard_bank.csv")


absa.to_csv("C:/GitHub/DS_PROJ/data/absa_jup.csv")


standard_bank = reformat(filter_totals(clean_names2(fill_totals(
    remove_empty(clean_names1(import_data(check_file("standard_bank.csv")), 
                                                                     bank_prefix="B_416061: "))))))
standard_bank
l_std = clean_liab(liabilities(standard_bank))
e_std = clean_equity(equity(standard_bank))
a_std = clean_assets(assets(standard_bank))

standard_bank = join_diff(l_std, e_std, a_std, bank="STANDARD_BANK")
standard_bank.head(1)


fnb = reformat(filter_totals(clean_names2(fill_totals(
    remove_empty(clean_names1(import_data(check_file("fnb.csv")), 
                                        bank_prefix=""))))))
fnb
l = clean_liab(liabilities(fnb))
e = clean_equity(equity(fnb))
a = clean_assets(assets(fnb))

fnb = join_diff(l, e, a, bank="FNB")
fnb.head(1)


nedbank = reformat(filter_totals(clean_names2(fill_totals(
    remove_empty(clean_names1(import_data(check_file("nedbank.csv")), 
                                                                     bank_prefix=""))))))
nedbank
l = clean_liab(liabilities(nedbank))
e = clean_equity(equity(nedbank))
a = clean_assets(assets(nedbank))

nedbank = join_diff(l, e, a, bank="NEDBANK")
nedbank.head(1)


investec = reformat(filter_totals(clean_names2(fill_totals(
    remove_empty(clean_names1(import_data(check_file("investec.csv")), 
                                                                     bank_prefix=""))))))
investec
l = clean_liab(liabilities(investec))
e = clean_equity(equity(investec))
a = clean_assets(assets(investec))

investec = join_diff(l, e, a, bank="INVESTEC")
investec.head(1)


capitec = reformat(filter_totals(clean_names2(fill_totals(
    remove_empty(clean_names1(import_data(check_file("capitec.csv")), 
                                                                     bank_prefix=""))))))
capitec
l = clean_liab(liabilities(capitec))
e = clean_equity(equity(capitec))
a = clean_assets(assets(capitec))

capitec = join_diff(l, e, a, bank="CAPITEC")
capitec.head(1)


def join_banks(df1, df2, df3, df4, df5, df6):
    df = reduce(lambda a, b: a.add(b, fill_value=0), [df1, df2, df3, df4, df5, df6])
    df = df.drop("Bank", axis=1)
    return(df)


banks = join_banks(absa, standard_bank, nedbank, fnb, capitec, investec)


banks.head(1)


banks.to_csv("./data/banks_data.csv")


shares = pd.read_csv("./data/share_prices.csv")
shares.head(1)


def wrangle_shares(df):
    df = shares[["TIME", "Value"]]
    df = df.set_index("TIME")
    df.index = pd.to_datetime(df.index, format="%Y-%m")
    df = df.diff()
    df = df.loc["1993-02-01":"2022-04-01"]
    return(df)


shares = wrangle_shares(shares)


InteractiveShell.ast_node_interactivity = "all"


shares.head(1)
banks.head(1)


shares.describe()
banks.describe()


InteractiveShell.ast_node_interactivity = "last"


share_corr = shares.join(banks).corr()[["Value"]]
plot_names = share_corr[share_corr["Value"]<0.10].index.to_list()
share_corr.sort_values("Value", key=abs, ascending=False)





import seaborn as sns

sns.pairplot(shares.join(banks), diag_kind="kde")
plt.show()


def quarter_delay(banks_df, shares_df):
    banks_df = banks_df.shift(-3, "MS")
    joined_df = shares_df.join(banks_df, how="inner")
    shares_df = joined_df[["Value"]]
    banks_df = joined_df.iloc[:,1:]
    return(banks_df, shares_df)


def month_delay(banks_df, shares_df):
    banks_df = banks_df.shift(-1, "MS")
    joined_df = shares_df.join(banks_df, how="inner")
    shares_df = joined_df[["Value"]]
    banks_df = joined_df.iloc[:,1:]
    return(banks_df, shares_df)


qbanks, qshares = quarter_delay(banks, shares)


mbanks, mshares = month_delay(banks, shares)


def compare_corr():
    share_corr = shares.join(banks).corr()[["Value"]]
    mshare_corr = mshares.join(mbanks).corr()[["Value"]]
    qshare_corr = qshares.join(qbanks).corr()[["Value"]]
    plot_names = share_corr[share_corr["Value"]<0.10].index.to_list()
    corrSTD = share_corr.sort_values("Value", key=abs, ascending=False)
    corrM = mshare_corr.sort_values("Value", key=abs)
    corrQ = qshare_corr.sort_values("Value", key=abs)
    corrjoin1 = corrSTD.join(corrM, lsuffix="_STD", rsuffix="_M")
    corrjoin1 = corrjoin1.join(corrQ, rsuffix="_Q")
    corrjoin1["M > Q"] = abs(corrjoin1["Value_M"]) > abs(corrjoin1["Value"])
    corrjoin1.loc["Total"] = corrjoin1.sum()
    return(corrjoin1)

compare_corr()


url='https://scikit-learn.org/stable/_static/ml_map.png'
Image(url, width=700, height=700)


from sklearn.linear_model import Ridge
from sklearn.preprocessing import Normalizer
from sklearn.model_selection import TimeSeriesSplit

X = np.array(banks)
y = np.array(shares["Value"])

scaler = Normalizer()
X = scaler.fit_transform(X)

tss = TimeSeriesSplit()

ridge = Ridge()
ridge.fit(X, y)


def coeff_exaplained(df, coeff):
    features_coeff = pd.DataFrame(columns=["Feature_Coeff"])
    features = df.columns.to_list()
    j = 0
    for i in coeff:
        #print(f"{features[j]}: {i}")
        features_coeff.loc[f"{features[j]}"] = [i]
        j+=1
    features_coeff.sort_values("Feature_Coeff", ascending=False, inplace=True, key=abs)
    return(features_coeff)


coeff_exaplained(banks, ridge.coef_)


from sklearn.model_selection import cross_val_score
def scores(X, y, model):
    '''Returns a dataframe containing the MSE & 
       R-Squared metrics of the given model, based
       on Time Series Data.'''
    tss_cv = tss.split(X)
    r_2 = cross_val_score(model, X, y, cv=tss_cv, scoring="r2")
    tss_cv = tss.split(X)
    mse = -cross_val_score(model, X, y, cv=tss_cv, scoring="neg_mean_squared_error")
    df = pd.DataFrame({ "Mean Squared Error": mse,
                        "R-Squared": r_2})
    df.loc["Avg:"] = df.mean()
    return(df)


scores(X, y, ridge)


from sklearn.ensemble import RandomForestRegressor


forest = RandomForestRegressor(oob_score=True)
forest.fit(X, y)


forest.feature_importances_


def features_exaplained(df, imp):
    features_imp = pd.DataFrame(columns=["Feature_Importance"])
    features = df.columns.to_list()
    j = 0
    for i in imp:
        #print(f"{features[j]}: {i}")
        features_imp.loc[f"{features[j]}"] = [i]
        j+=1
    features_imp.sort_values("Feature_Importance", ascending=False, inplace=True)
    features_imp.loc["Total"] = features_imp.sum()
    features_imp.loc["(The Out of Bag Score Returns: )"] = [forest.oob_score_]
    return(features_imp)


features_exaplained(df=banks, imp=forest.feature_importances_)


scores(X, y, forest)


from sklearn.base import clone 

def imp_df(column_names, importances):
    data = {
        'Feature': column_names,
        'Importance': importances,
    }
    df = pd.DataFrame(data) \
        .set_index('Feature') \
        .sort_values('Importance', ascending=False)

    return(df)

def feat_eval_iter(model, X, y, random_state = 42):
    
    # clone the model to have the exact same specification as the one initially trained
    forest_clone = clone(model)
    # set random_state for comparability
    forest_clone.random_state = random_state
    # training and scoring the benchmark model
    forest_clone.fit(X, y)
    benchmark_score = forest_clone.score(X, y)
    # list for storing feature importances
    importances = []
    
    # iterating over all columns and storing feature importance (difference between benchmark and new model)
    for col in X.columns:
        forest_clone = clone(model)
        forest_clone.random_state = random_state
        forest_clone.fit(X.drop(col, axis = 1), y)
        drop_col_score = forest_clone.score(X.drop(col, axis = 1), y)
        importances.append(benchmark_score - drop_col_score)

    importances_df = imp_df(X.columns, importances)
    
    
    
    return(importances_df)


df_imp = feat_eval_iter(forest, banks, y)


df_imp.to_csv("./data/feature_imp_select.csv")
df_imp


Image("./figures/Feature_imp_plot.png", width=700, height=700)


def reduce_banks(df):
    df1 = df_imp[df_imp["Importance"]>0]
    df1 = df1.reset_index()
    df1 = df[df1["Feature"].to_list()]
    return(df1)


reduced_banks = reduce_banks(banks)


forest_reduced = RandomForestRegressor(oob_score=True)
X_reduced = np.array(reduce_banks(banks))
forest_reduced.fit(X_reduced, y)


features_exaplained(df=reduced_banks, imp=forest_reduced.feature_importances_)


scores(X_reduced, y, forest_reduced)


X_m = reduce_banks(mbanks)
y_m = np.array(mshares["Value"])

forest_m = RandomForestRegressor(oob_score=True)
forest_m.fit(X_m, y_m)


features_exaplained(df=X_m, imp=forest_m.feature_importances_)


scores(X_m, y_m, forest_m)



